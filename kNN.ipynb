{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "27685cc8-811e-4533-87f2-e52d73ca2c96",
      "metadata": {
        "id": "27685cc8-811e-4533-87f2-e52d73ca2c96"
      },
      "source": [
        "# $k$ Nearest Neighbor\n",
        "## Foundations of Machine Learning\n",
        "## `! git clone https://www.github.com/DS3001/knn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "505d5dd8-c57e-44bd-be4a-870e9c5fc4aa",
      "metadata": {
        "id": "505d5dd8-c57e-44bd-be4a-870e9c5fc4aa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744a791d-c0e8-42d8-bb00-c47813a31bb2",
      "metadata": {
        "id": "744a791d-c0e8-42d8-bb00-c47813a31bb2"
      },
      "source": [
        "## Algorithms and Models\n",
        "- The analysis we've done up to now is about cleaning and summarizing the properties of data\n",
        "- We want to pivot to using the data to make predictions\n",
        "- In machine learning, there are two key concepts:\n",
        "  - *Algorithms*: A process that converts inputs into results\n",
        "  - *Models*: A mathematical abstraction or simplification intended to mimic real phenomena\n",
        "- **Machine learning**: *We (1) use algorithms to fit models to data, and then (2) use those fitted models to make predictions about future outcomes for new data*\n",
        "- The goal is to produce a data-driven tool for predictive purposes\n",
        "- There is a lot to learn, and the steps will become more natural as we fit and use more models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e89f51-4fc1-45b4-86bb-773f96025e9b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "04e89f51-4fc1-45b4-86bb-773f96025e9b"
      },
      "source": [
        "## Two Algorithmic Steps\n",
        "- There's two steps: (1) Fitting the model and (2) Making the predictions\n",
        "- We'll introduce the model by working backwards: Given a fitted model, how do you make predictions? Understanding how models work gives you insights into how to build them.\n",
        "- With an understanding of how the model works, we'll then take a step back: How do you train/fit/tune/etc. the model? This is a *training algorithm* or *optimization algorithm*.\n",
        "- In common practice, the difference between machine learning and statistics is that ML is focused on *prediction* and less on *inference* or *causality* --- the goal is to build predictive models, not estimate them and engage in *hypothesis testing* (unbiased/consistent estimates are not necessarily [often not] good predictive models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f283d5-a33e-44c0-98c0-924dabe21257",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "92f283d5-a33e-44c0-98c0-924dabe21257"
      },
      "source": [
        "## Data Examples\n",
        "- Car Price (`USA_cars_datasets.csv`): Based on car attributes ($X$), predict its transaction price ($y\\ge 0$)\n",
        "- Heart Failure (`heart_failure_clinical_records_dataset.csv`): Based on patient characteristics ($X$), predict whether they die ($y=0,1$)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ee7903-6c69-4c7b-8c06-7dd83a28b5f1",
      "metadata": {
        "id": "a7ee7903-6c69-4c7b-8c06-7dd83a28b5f1"
      },
      "source": [
        "## Data in General\n",
        "- We have a clean $N \\times (M+1)$ matrix $D$ of data; rows $i$ are observations and columns $j$ are variables\n",
        "- One variable is be the *target*, *dependent*, *outcome*, *prediction*, *labeled* variable, which is an $N \\times 1$ vector $y$\n",
        "- All the other variables are potential *feature*, *explanatory*, *independent*, *control*, *covariate*, *predictor* variables, which is an $N \\times M$ matrix $X$\n",
        "- We'll refer to the $i$-th row/observation of $X$ by $x_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea966e2c-a2f5-4a2e-a38c-bf4ae6321d55",
      "metadata": {
        "id": "ea966e2c-a2f5-4a2e-a38c-bf4ae6321d55"
      },
      "source": [
        "## Euclidean Distance\n",
        "- Distance is at the core of many of our models (kNN, kMC, kernel methods), so let's review the basic defintions before we start with kNN\n",
        "- Take two numbers, like 5 and 7. What's the distance between them? You can start by subtracting them, but the order then seems to matter: $5-7=-2$ and $7-5=2$. So we square that difference, and then take the square root again to convert to the absolute value:\n",
        "$$\n",
        "d(z,w) = \\sqrt{(z-w)^2} = |z-w|\n",
        "$$\n",
        "    - That gives us $\\sqrt{(5-7)^2} = \\sqrt{(-2)^2} = \\sqrt{4}=2$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d4a61ed-3ddb-4221-a987-8d4ac57bb1fe",
      "metadata": {
        "id": "6d4a61ed-3ddb-4221-a987-8d4ac57bb1fe"
      },
      "source": [
        "## Distance in Multiple Dimensions\n",
        "- Distance on the real line between two numbers is intuitive, but what about in multiple dimensions?\n",
        "- Take `z = (3,5,7)` and `w=(2,-4,6)`\n",
        "- Then the squared differences are $(3-2)^2=1$, $(5--4)^2=81$, and $(7-6)^2=1$\n",
        "- The sum is 83\n",
        "- The square root is 9.110\n",
        "```\n",
        "np.sqrt((3-2)**2+(5--4)**2+(7-6)**2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda35211-2ef3-429c-8f37-8eb128c2c243",
      "metadata": {
        "id": "cda35211-2ef3-429c-8f37-8eb128c2c243"
      },
      "source": [
        "## Euclidean Distance\n",
        "- What about the distance between two vectors $z = (z_1, z_2, ..., z_L)$ and $w = (w_1, w_2, ..., w_L)$? For example, two rows of a dataframe?\n",
        "- To measure the *(Euclidean) distance* between two vectors, we do \"the same thing\":\n",
        "    1. For each dimension $1, 2, ..., L$, compute the squared difference, $(z_\\ell-w_\\ell)^2$\n",
        "    2. Sum the squared differences, $S =(z_1-w_1)^2+ ... + (z_L-w_L)^2$\n",
        "    3. Take the square root of the sum, $\\sqrt{S}$\n",
        "- More compactly:\n",
        "$$\n",
        "d(z,w) = \\sqrt{\\underbrace{(z_1-w_1)^2}_{\\text{First Dimension}} + ... + \\underbrace{(z_L-w_L)^2}_{\\text{$L$-th Dimension}}} =  \\sqrt{\\sum_{\\ell=1}^L (z_{\\ell} - w_{\\ell})^2} = ||z-w||\n",
        "$$\n",
        "Again, distance is such a ubiquitous idea that there are many notations for it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2dfd29c-b446-44e0-a50b-d0ab3dbb37f0",
      "metadata": {
        "id": "d2dfd29c-b446-44e0-a50b-d0ab3dbb37f0"
      },
      "source": [
        "## Other Kinds of Distance\n",
        "- There are many ways to think of the distance between two objects, and they all have strengths and weaknesses:\n",
        "    - Sup/Max norm: $ d_\\infty(z,w) = \\max_{i} |z_i-w_i|$ is useful for worst-case situations\n",
        "    - Euclidean norm: $ d_2(z,w) = \\sqrt{\\sum_{\\ell=1}^L (z_{\\ell} - w_{\\ell})^2}$, the standard distance metric\n",
        "    - Manhattan norm: $d_1(z,w) = \\sum_{\\ell=1}^L |z_\\ell - w_\\ell|$ is more robust to outliers than Euclidean\n",
        "    - $p$-norm: $ d_p(z,q) = \\left( \\sum_{\\ell=1}^L |z_\\ell-w_\\ell|^p \\right)^{1/p}$ generalizes all the previous cases ($p=2$ for Euclidean, $p=\\infty$ for max, $p=1$ for Manhattan)\n",
        "- It often depends on context: How do you measure the social distance between people?\n",
        "- This is extremely important for many algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bdf125e-0114-476b-99a5-02843074c725",
      "metadata": {
        "id": "4bdf125e-0114-476b-99a5-02843074c725"
      },
      "source": [
        "## k Nearest Neighbor\n",
        "- The following claim seems reasonable: \"Suppose that a target variable $y$ follows from features $x$. If $x$ and $x'$ are close together, then $y$ and $y'$ are probably also close together.\" **On some level, this is the core premise of machine learning.**\n",
        "- If that's true, then making a prediction $\\hat{y}$ for a new case $\\hat{x}$ could be done by finding the values of $x$ in the data close to $\\hat{x}$, and averaging those values\n",
        "- We'll operationalize this idea more precisely in the subsequent slides"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dcbbdd5",
      "metadata": {
        "id": "7dcbbdd5"
      },
      "source": [
        "## Car Data\n",
        "- The data are from the Environmental Protection Agency, and contain some interesting variables:\n",
        "    - Manufacturer: Honda, BMW, etc.\n",
        "    - EPA class: SUV, Midsize Car, etc.\n",
        "    - Footprint: A measure of carbon emissions averaged over time\n",
        "    - baseline price: Market price of the vehicle\n",
        "    - baseline mpg: manufacturer's claim of miles per gallon\n",
        "    - baseline sales: Predicted sales of vehicle\n",
        "- We want to start by doing some EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d0f92195",
      "metadata": {
        "id": "d0f92195",
        "outputId": "8ca6b02a-44fd-4ed7-8b57-dc647b3f0b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/cars_env.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a8fa505b2e8d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/cars_env.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/cars_env.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('./data/cars_env.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1408ff73",
      "metadata": {
        "id": "1408ff73"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef0a994",
      "metadata": {
        "id": "9ef0a994"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(data=df,x='baseline price')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7201fc60",
      "metadata": {
        "id": "7201fc60"
      },
      "outputs": [],
      "source": [
        "# Drop extremely expensive cars:\n",
        "q90 = np.quantile( df['baseline price'],.9) # Compute the .9 quantile\n",
        "# drop 90 % of data\n",
        "print(q90)\n",
        "keep = df['baseline price'] < q90  # Logical condition asserting price < .9 quantile\n",
        "df = df.loc[keep,:] # Use locator function to filter on a Boolean conditional\n",
        "df.describe()\n",
        "sns.kdeplot(data=df,x='baseline price')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7aa8bc",
      "metadata": {
        "id": "ca7aa8bc"
      },
      "outputs": [],
      "source": [
        "df['EPA class'].value_counts() # This is too fine-grained a set of distinctions for our purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b685b5",
      "metadata": {
        "id": "21b685b5"
      },
      "outputs": [],
      "source": [
        "# Simplify the vehicle classification scheme:\n",
        "# aggregate values (group them together)\n",
        "df['class'] = df['EPA class']\n",
        "df['class'] = df['class'].replace(['MIDSIZE CARS','COMPACT CARS','SUBCOMPACT CARS','TWO SEATERS','LARGE CARS'],'car')\n",
        "df['class'] = df['class'].replace(['SMALL STATION WAGONS','MIDSIZE STATION WAGONS'],'station wagon')\n",
        "df['class'] = df['class'].replace(['STANDARD PICKUP TRUCKS','SMALL PICKUP TRUCKS'],'truck')\n",
        "df['class'] = df['class'].replace(['VANS','MINIVAN'],'van')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c23d69f",
      "metadata": {
        "id": "3c23d69f"
      },
      "outputs": [],
      "source": [
        "df['class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d423b15c",
      "metadata": {
        "id": "d423b15c"
      },
      "outputs": [],
      "source": [
        "df.loc[:,['baseline price','EPA class']].groupby('EPA class').describe() # Baseline price by EPA vehicle class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9828da6",
      "metadata": {
        "id": "d9828da6"
      },
      "outputs": [],
      "source": [
        "df.loc[:,['baseline price','class']].groupby('class').describe() # Baseline price by simplified vehicle class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26089aa2",
      "metadata": {
        "id": "26089aa2"
      },
      "outputs": [],
      "source": [
        "# Plot footprint against price\n",
        "this_plot = sns.scatterplot(data=df,x='footprint',y='baseline price',\n",
        "                            hue='baseline sales',style='class',\n",
        "                           palette = 'crest')\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0ba3d71a",
      "metadata": {
        "id": "0ba3d71a",
        "outputId": "7953f3f6-7dfc-4509-fd8c-ed4da62a18cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4507c4fdb643>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot sales against price\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m this_plot = sns.scatterplot(data=df,x='baseline price',y='baseline sales',hue='footprint',style='class',\n\u001b[0m\u001b[1;32m      3\u001b[0m                            palette = 'crest')\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_legend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"upper left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_to_anchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Move legend off the plot canvas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Plot sales against price\n",
        "this_plot = sns.scatterplot(data=df,x='baseline price',y='baseline sales',hue='footprint',style='class',\n",
        "                           palette = 'crest')\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59eb201d",
      "metadata": {
        "id": "59eb201d"
      },
      "outputs": [],
      "source": [
        "df.loc[:,['footprint','class']].groupby('class').describe() # Footprint by simplified vehicle class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a600345",
      "metadata": {
        "id": "4a600345"
      },
      "source": [
        "## Prediction: Regression and Classification\n",
        "- Let's focus on the graph below: a scatterplot of `baseline price` and `footprint`, with sales coded by hue and vehicle class coded by the marker\n",
        "- Let's stipulate that we're interested in two questions:\n",
        "    - Conditional on price and footprint, what are sales likely to be? (numeric outcome)\n",
        "    - Conditional on price and footprint, what class of vehicle is it likely to be? (categorical outcome)\n",
        "- Using data to predit a numeric outcome is called **regression**, and using data to predict a categorical outcome is called **classification**\n",
        "- We often denote the **target variable** or **outcome variable** by $y$ and a prediction by $\\hat{y}$, and we denote the **features** or **covariates** by $x$\n",
        "- The two approaches -- regression and classification -- have some substantial differences that we'll discuss as we go along"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5786f0e",
      "metadata": {
        "id": "a5786f0e"
      },
      "outputs": [],
      "source": [
        "# Plot footprint against price\n",
        "this_plot = sns.scatterplot(data=df,x='footprint',y='baseline price',\n",
        "                            hue='baseline sales',style='class',\n",
        "                           palette = 'crest')\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cdc4295-476f-4fdc-a0fb-ae9e7d68d194",
      "metadata": {
        "id": "5cdc4295-476f-4fdc-a0fb-ae9e7d68d194"
      },
      "source": [
        "## $k$ Nearest Neighbor Prediction Algorithm (Regression)\n",
        "- Consider a new case $\\hat{x} = (\\hat{x}_1,...,\\hat{x}_L)$. We want to make a guess of what value it will likely take, $\\hat{y}$, given existing data $D = (X,y)$\n",
        "- Suppose the outcome variable $y$ is numeric: Price, weight, time, decibels, etc.\n",
        "- The *$k$ Nearest Neighbor Regression Algorithm* is:\n",
        "  1. Compute the distance from $\\hat{x}$ to each observation $x_i$\n",
        "  2. Find the $k$ \"nearest neighbors\" $x_1^*$, $x_2^*$, ..., $x_K^*$ to $\\hat{x}$ in the data, with outcomes $y_{1}^*$, $y_2^*$, ..., $y_K^*$\n",
        "  3. Compute the average nearest neighhor outcome as the prediction for $\\hat{x}$:\n",
        "  -the modal case among the nieghbors (cheater bc 52% chance used chatgpt)\n",
        "  - a probability of each class occurring, proportional to the frequency in the nieghbors  (52% used chat but also 48% didn't use it)\n",
        "\n",
        "$$\n",
        "  \\hat{y} = \\dfrac{y_1^* + y_2^* + ... + y_K^* }{K} =  \\dfrac{1}{K} \\sum_{k=1}^K y_k^*\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c2e511-6f8c-4ac9-9f23-c3833f3f1846",
      "metadata": {
        "id": "91c2e511-6f8c-4ac9-9f23-c3833f3f1846"
      },
      "outputs": [],
      "source": [
        "def knn_reg(x_hat,gdf,K):\n",
        "    # Compute distances between x_hat and the data:\n",
        "    squared_differences = (x_hat - gdf.loc[:,['x1','x2']])**2\n",
        "    distances = np.sum( squared_differences , axis = 1)\n",
        "    # Find k smallest values in dist:\n",
        "    neighbors = np.argsort(distances)[:K].tolist()\n",
        "    # Find y values for the nearest neighbors:\n",
        "    y_star = gdf['y'].iloc[neighbors].tolist()\n",
        "    # Average neighbor values to get prediction:\n",
        "    y_hat = np.mean(y_star)\n",
        "    # Return a dictionary of computed values of interest:\n",
        "    return({'y_hat':y_hat, 'y_star':y_star, 'neighbors':neighbors})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74605d7d",
      "metadata": {
        "id": "74605d7d"
      },
      "outputs": [],
      "source": [
        "# Cleaning the data a bit; we'll come back to this later\n",
        "gdf = df.loc[:,['footprint', 'baseline price',  'baseline sales', 'class',]]\n",
        "gdf = gdf.rename(columns={'footprint':'x1', 'baseline price':'x2', 'class':'g','baseline sales':'y'})\n",
        "\n",
        "def maxmin(z):\n",
        "    w = (z-np.min(z))/(np.max(z)-np.min(z))\n",
        "    return w\n",
        "\n",
        "gdf['x1'] = maxmin(gdf['x1'])\n",
        "gdf['x2'] = maxmin(gdf['x2'])\n",
        "gdf['y'] = maxmin(gdf['y'])\n",
        "\n",
        "gdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4a6a22-2206-4280-91f6-7d717a797a89",
      "metadata": {
        "id": "4c4a6a22-2206-4280-91f6-7d717a797a89"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "x_hat = np.array((.2,.2)) # Example prediction case\n",
        "knn = knn_reg(x_hat, gdf, 3) # Call our function\n",
        "print(knn)\n",
        "\n",
        "# Plot results:\n",
        "gdf2 = gdf.copy() # Notice the .copy(); why?\n",
        "hat_row = [ x_hat[0], x_hat[1],knn['y_hat'],'hat']\n",
        "gdf2.loc[len(gdf2)] = hat_row\n",
        "this_plot = sns.scatterplot(data=gdf2,x='x1',y='x2',hue='y',style='g',palette='crest') # Plot the fake data\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d66adf-3941-4e63-ae97-ce6d001aa197",
      "metadata": {
        "id": "13d66adf-3941-4e63-ae97-ce6d001aa197"
      },
      "source": [
        "## $k$ Nearest Neighbor Prediction Algorithm (Classification)\n",
        "- Consider a new case $\\hat{x} = (\\hat{x}_1,...,\\hat{x}_L)$. We want to make a guess of what **class** it will likely take, $\\hat{g}$, given existing data $D = (X,g)$\n",
        "- Suppose the outcome variable $y$ is categorical: Model, species, country, etc.\n",
        "- The *$k$ Nearest Neighbor Classification Algorithm* is:\n",
        "  1. Compute the distance from $\\hat{x}$ to each observation $x_i$\n",
        "  2. Find the $k$ \"nearest neighbors\" $x_1^*$, $x_2^*$, ..., $x_K^*$ to $\\hat{x}$ in the data, with outcomes $g_{1}^*$, $g_2^*$, ..., $g_K^*$\n",
        "  3. Compute either\n",
        "        - The modal case among the neighbors, $g_{1}^*$, $g_2^*$, ..., $g_K^*$\n",
        "        - A probability of each class occuring, proportional to the frequency in the neighbors $g_{1}^*$, $g_2^*$, ..., $g_K^*$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea18e862-79c2-40e1-920b-1e3e7d351816",
      "metadata": {
        "id": "ea18e862-79c2-40e1-920b-1e3e7d351816"
      },
      "outputs": [],
      "source": [
        "# A simple implementaton of kNN for classification:\n",
        "def knn_class(x_hat,gdf,K):\n",
        "    # Compute distances between x_hat and the data:\n",
        "    squared_differences = (x_hat - gdf.loc[:,['x1','x2']])**2\n",
        "    distances = np.sum( squared_differences , axis = 1)\n",
        "    # Find k smallest values in dist:\n",
        "    neighbors = np.argsort(distances)[:K].tolist()\n",
        "    # Find g values for the nearest neighbors:\n",
        "    g_star = gdf['g'].iloc[neighbors]\n",
        "    # Modal class:\n",
        "    g_dist = g_star.value_counts()/K\n",
        "    g_modal = g_dist.index[g_dist.argmax()]\n",
        "    # Return a dictionary of computed values of interest:\n",
        "    return({'neighbors':neighbors, 'g_star':g_star, 'g_dist':g_dist, 'g_modal':g_modal, })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b42901-cd36-4ebf-9bf5-4fb9e06e3193",
      "metadata": {
        "id": "14b42901-cd36-4ebf-9bf5-4fb9e06e3193"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "x_hat = np.array((.8,.2)) # Example prediction case\n",
        "knn = knn_class(x_hat, gdf, 3) # Call our function\n",
        "print(knn)\n",
        "\n",
        "gdf2 = gdf.copy()\n",
        "hat_row = [ x_hat[0], x_hat[1],100, knn['g_modal']]\n",
        "print(hat_row)\n",
        "gdf2.loc[len(gdf2)] = hat_row\n",
        "this_plot = sns.scatterplot(data=gdf2,x='x1',y='x2',hue='y',style='g', palette='crest') # Plot the fake data\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63de08f-ca09-4ec5-93ef-474e5d566715",
      "metadata": {
        "id": "c63de08f-ca09-4ec5-93ef-474e5d566715"
      },
      "source": [
        "## SciKit-Learn\n",
        "- Unless we are doing something tailored to a particular task, we generally don't want to code our own algorithms: It is time consuming, and existing implementations are typically more efficient or robust than what we would create (but there is a lot of value in coding your own algorithms and estimators)\n",
        "- The most popular Python machine learning library is called **SciKit-Learn**\n",
        "- You typically import it as `from sklearn.<model class> import <model name>`, where `<model class>` is the set of related models and `<model name>` is the desired algorithm\n",
        "    - In particular, `from sklearn.neighbors import KNeighborsRegressor` and `from sklearn.neighbors import KNeighborsClassifier`\n",
        "- The workflow with `sk` is that you use it to\n",
        "  1. Create an untrained model object with a fixed $k$: `model = kNNRegression(n_neighbors=k)`\n",
        "  2. Fit that object to the data, $(X,y)$: `fitted_model = model.fit(X,y)`\n",
        "  3. Use the fitted object to make predictions for new cases $\\hat{x}$: `y_hat = fitted_model.predict(x_hat)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d2a9118-552d-4519-9bca-d4c09f2b42c9",
      "metadata": {
        "id": "7d2a9118-552d-4519-9bca-d4c09f2b42c9"
      },
      "source": [
        "## Fitting a Model with kNN\n",
        "- To prepare to fit the model we have to do a few things:\n",
        "    - Import the model we want to use: `from sklearn.neighbors import KNeighborsRegressor`\n",
        "    - Create a matrix `X` of features and a vector `y` for the target variable\n",
        "    - **Scale the features in `X` so their values are comparable**\n",
        "- Once we have our desired model imported and the data prepared as $D=(y,X)$, we can fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e2b7718-66c4-440b-849a-d3402216cfa2",
      "metadata": {
        "id": "4e2b7718-66c4-440b-849a-d3402216cfa2"
      },
      "source": [
        "## Feature/Variable Normalization/Scaling\n",
        "- Problem: the distances in $k$NN depends on the \"size/scale\" of variables. Multiplying one variable by 10 or dividing by 100 (but not the others) will likely change your predictions\n",
        "- Solution: We often **scale** or **normalize** variables to reduce the extent to which their relative values change the performance of the model\n",
        "- There are a variety of approaches to this, but the simplest is **MaxMin Normalization**: For a vector $x$ and element $x_i$,\n",
        "$$\n",
        "u_i = \\dfrac{x_i - \\min(x)}{\\max(x)-\\min(x)}\n",
        "$$\n",
        "- You do not have to normalize the outcome variable, $y$, since we don't look at any distances related to $y$ in fitting or using the model (in some algorithms, you would want to)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09848b3-daa9-4546-9320-3f5c0e91cb82",
      "metadata": {
        "id": "a09848b3-daa9-4546-9320-3f5c0e91cb82"
      },
      "source": [
        "## The `.apply()` Method\n",
        "- We'll make a maxmin scaler function:\n",
        "```\n",
        "def maxmin(z):\n",
        "    z = (z-min(z))/(max(z)-min(z))\n",
        "    return(z)\n",
        "```\n",
        "- But then we want to use it on each column of the data `X`\n",
        "- Instead of a `for` loop, we can instead use the `df.apply(fcn)` method to apply the function `fcn` to each column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61908aa5-42a8-45fe-bdb9-d14d096fd8f8",
      "metadata": {
        "id": "61908aa5-42a8-45fe-bdb9-d14d096fd8f8"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor # Import the kNN regression tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc35d398-9753-499e-b59b-fe4fa1bf1084",
      "metadata": {
        "id": "cc35d398-9753-499e-b59b-fe4fa1bf1084"
      },
      "outputs": [],
      "source": [
        "# Select features/target variable for analysis:\n",
        "y = df['baseline sales'] # Create target variable vector\n",
        "X = df.loc[:,['baseline price','footprint']] # Create feature matrix\n",
        "print(X.head())\n",
        "print(X.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27bc9bd-d96b-4eef-a63f-626bbc3f5ca5",
      "metadata": {
        "id": "f27bc9bd-d96b-4eef-a63f-626bbc3f5ca5"
      },
      "outputs": [],
      "source": [
        "# Maxmin normalization function:\n",
        "def maxmin(z):\n",
        "    z = (z-min(z))/(max(z)-min(z))\n",
        "    return(z)\n",
        "\n",
        "# Apply maxmin to each column of X to get U:\n",
        "X = X.apply(maxmin)\n",
        "print(X.head())\n",
        "print(X.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6656347e-2432-45b6-bd40-74b3f2c5d0e7",
      "metadata": {
        "id": "6656347e-2432-45b6-bd40-74b3f2c5d0e7"
      },
      "outputs": [],
      "source": [
        "# Fit the model:\n",
        "k=3\n",
        "model = KNeighborsRegressor(n_neighbors=k) # Create a sk model for k=3\n",
        "fitted_model = model.fit(X,y) # Train the model on our data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c8ddd6-b517-4f44-afbd-6b8e00c6ac99",
      "metadata": {
        "id": "e9c8ddd6-b517-4f44-afbd-6b8e00c6ac99"
      },
      "source": [
        "## Prediction\n",
        "- Now that we have a fitted model, we can make predictions with it\n",
        "- You can pass dataframes of $\\hat{x}$ values into the `.predict(x_hat)` method of a fitted model\n",
        "- This returns a set of predictions $\\hat{y}$\n",
        "- Let's plot the model predictions on a grid to see what it does\n",
        "- The next code chunk is just a tool to make a picture, you don't need to follow each line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab3254a-3d53-4156-924d-c76708824723",
      "metadata": {
        "id": "5ab3254a-3d53-4156-924d-c76708824723"
      },
      "outputs": [],
      "source": [
        "N_x = 100 # Coarseness of x variable\n",
        "N_y = 100 # Coarseness of y variable\n",
        "total = N_x*N_y # Total number of points to plot\n",
        "\n",
        "grid_x = np.linspace(0,1,N_x) # Create a grid of x values\n",
        "grid_y = np.linspace(0,1,N_y) # Create a grid of y values\n",
        "\n",
        "xs, ys = np.meshgrid(grid_x,grid_y) # Explode grids to all possible pairs\n",
        "X = xs.reshape(total) # Turns pairs into vectors\n",
        "Y = ys.reshape(total) # Turns pairs into vectors\n",
        "\n",
        "x_hat = pd.DataFrame({'baseline price':X,'footprint':Y}) # Create a dataframe of points to plot\n",
        "y_hat = fitted_model.predict(x_hat) # Fit the model to the points\n",
        "x_hat['Predicted Sales'] = y_hat # Add new variable to the dataframe\n",
        "# Create seaborn plot:\n",
        "this_plot = sns.scatterplot(data=x_hat,x='baseline price',y='footprint',\n",
        "                            hue='Predicted Sales', palette = 'crest', linewidth=0)\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1825af5-de6d-484d-a301-82167e32e7ff",
      "metadata": {
        "id": "c1825af5-de6d-484d-a301-82167e32e7ff"
      },
      "source": [
        "## Changing the number of neighbors\n",
        "- We'll look at how to pick $k$ in a few slides\n",
        "- Before we do that, how does the predictive model change as $k$ changes?\n",
        "- Basically, the decision boundaries will get \"softer\" and the predictions will become less binary\n",
        "- At some point, the model turns to mush: Instead of using local information, you're using \"most of the data\", and the predictions will tend towards the sample average\n",
        "- It is often useful to push models to extremes to see how they \"break\"\n",
        "- **Underfitting** occurs when your model is too simple to reliably explain the phenomenon you are interested in, and **overfitting** occurs when your model is too complex to reliably explain the phenomenon you are interested in\n",
        "- Notice the minimum and maximum predicted values in the legend, and the coarseness of the decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034506c8-cedb-497d-9f4c-da3d5d7a00f4",
      "metadata": {
        "id": "034506c8-cedb-497d-9f4c-da3d5d7a00f4"
      },
      "outputs": [],
      "source": [
        "def statics(k=3,data=df): # This function replicates our previous work, with k as an input\n",
        "    y = df['baseline sales'] # Create target variable vector\n",
        "    X = df.loc[:,['baseline price','footprint']] # Create feature matrix\n",
        "    # Normalize:\n",
        "    X = X.apply(maxmin)\n",
        "    model = KNeighborsRegressor(n_neighbors=k) # Create a sk model for k=3\n",
        "    fitted_model = model.fit(X,y) # Train the model on our data\n",
        "    N_x = 100 # Coarseness of x variable\n",
        "    N_y = 100 # Coarseness of y variable\n",
        "    total = N_x*N_y # Total number of points to plot\n",
        "    grid_x = np.linspace(0,1,N_x) # Create a grid of x values\n",
        "    grid_y = np.linspace(0,1,N_y) # Create a grid of y values\n",
        "    xs, ys = np.meshgrid(grid_x,grid_y) # Explode grids to all possible pairs\n",
        "    X = xs.reshape(total) # Turns pairs into vectors\n",
        "    Y = ys.reshape(total) # Turns pairs into vectors\n",
        "    x_hat = pd.DataFrame({'baseline price':X,'footprint':Y}) # Create a dataframe of points to plot\n",
        "    y_hat = fitted_model.predict(x_hat) # Fit the model to the points\n",
        "    x_hat['Predicted Sales'] = y_hat # Add new variable to the dataframe\n",
        "    # Create seaborn plot:\n",
        "    this_plot = sns.scatterplot(data=x_hat,x='baseline price',y='footprint',\n",
        "                                hue='Predicted Sales',palette='crest',linewidth=0)\n",
        "    sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5175de-022a-4afd-a442-641b43b08f4d",
      "metadata": {
        "id": "7d5175de-022a-4afd-a442-641b43b08f4d"
      },
      "outputs": [],
      "source": [
        "statics(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad7690b-a814-4869-919b-f8dcf89a4254",
      "metadata": {
        "id": "4ad7690b-a814-4869-919b-f8dcf89a4254"
      },
      "outputs": [],
      "source": [
        "statics(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6df2f66-ed80-4b86-acb8-4a55dac9496d",
      "metadata": {
        "id": "c6df2f66-ed80-4b86-acb8-4a55dac9496d"
      },
      "outputs": [],
      "source": [
        "statics(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a73983d6-e007-4ee5-a4db-e1146354e584",
      "metadata": {
        "id": "a73983d6-e007-4ee5-a4db-e1146354e584"
      },
      "outputs": [],
      "source": [
        "statics(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89ff96a9-a4b1-424d-adf7-ae6c47df4a11",
      "metadata": {
        "id": "89ff96a9-a4b1-424d-adf7-ae6c47df4a11"
      },
      "outputs": [],
      "source": [
        "statics(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655f1581-76d7-4755-a1a6-106ebf451afa",
      "metadata": {
        "id": "655f1581-76d7-4755-a1a6-106ebf451afa"
      },
      "outputs": [],
      "source": [
        "statics(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f03a034f-fe2f-4f39-965a-b3af8075ba3e",
      "metadata": {
        "id": "f03a034f-fe2f-4f39-965a-b3af8075ba3e"
      },
      "outputs": [],
      "source": [
        "statics(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ce0460-203f-4f34-a9c1-cd705cae3aeb",
      "metadata": {
        "id": "24ce0460-203f-4f34-a9c1-cd705cae3aeb"
      },
      "outputs": [],
      "source": [
        "statics(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68240238-bc6a-4255-803a-736a820d468d",
      "metadata": {
        "id": "68240238-bc6a-4255-803a-736a820d468d"
      },
      "outputs": [],
      "source": [
        "statics(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40bee624-3394-414e-ac2e-0a6e782ffbcc",
      "metadata": {
        "id": "40bee624-3394-414e-ac2e-0a6e782ffbcc"
      },
      "outputs": [],
      "source": [
        "statics(470)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd537e0-51da-4086-a8d8-b00675438ba7",
      "metadata": {
        "id": "8fd537e0-51da-4086-a8d8-b00675438ba7"
      },
      "source": [
        "## Comparative Statics on $k$\n",
        "- As $k$ goes up, the prediction regions get fuzzier\n",
        "- The range of predicted values shrinks, because you're averaging over more values and extremes get moderated\n",
        "- Eventually, you're averaging over so many neighbors that you make roughly the same predictions for everyone\n",
        "- So the model can be unrealistically precise ($k$ is very small) or unrealistically imprecise ($k$ is very large)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a068e5-e237-4b4d-adb2-2d38821d96c3",
      "metadata": {
        "id": "69a068e5-e237-4b4d-adb2-2d38821d96c3"
      },
      "source": [
        "## What happens if we fail to scale?\n",
        "- Well, it doesn't \"work\", it looks really bad: This is an explanation of what not to do\n",
        "- Since `baseline price` is in tens of thousands and `footprint` is in tens, we get \"vertical\" equivalence classes of predictions: The distance between one variable are uniformly larger than distances between the other, so there's immense pressure to pick neighbors with similar prices rather than footprints\n",
        "- But we want to use both dimensions somewhat equally.\n",
        "- When using algorithms based on distance metrics, you really have to make sure that the way you conceive of \"distance\" across dimensions makes sense.\n",
        "- (An alternative is using smoothing to weight neighbors, like a kernel density plot, rather than a hard boundary that picks specific neighbors, like a histogram.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e46fd3-9c6a-4cc3-b883-fbfc35784c9f",
      "metadata": {
        "id": "41e46fd3-9c6a-4cc3-b883-fbfc35784c9f"
      },
      "outputs": [],
      "source": [
        "# Select features/target variable for analysis:\n",
        "y = df['baseline sales'] # Create target variable vector\n",
        "X = df.loc[:,['baseline price','footprint']] # Create feature matrix\n",
        "model = KNeighborsRegressor(n_neighbors=3) # Create a sk model for k=3\n",
        "fitted_model = model.fit(X,y) # Train the model on our data\n",
        "\n",
        "# Make a plot of the decision rule:\n",
        "N_x = 100 # Coarseness of x variable\n",
        "N_y = 100 # Coarseness of y variable\n",
        "total = N_x*N_y # Total number of points to plot\n",
        "xmin = np.min(X.iloc[:,0])\n",
        "xmax = np.max(X.iloc[:,0])\n",
        "ymin = np.min(X.iloc[:,1])\n",
        "ymax = np.max(X.iloc[:,1])\n",
        "grid_x = np.linspace(xmin,xmax,N_x) # Create a grid of x values\n",
        "grid_y = np.linspace(ymin,ymax,N_y) # Create a grid of y values\n",
        "xs, ys = np.meshgrid(grid_x,grid_y) # Explode grids to all possible pairs\n",
        "X = xs.reshape(total) # Turns pairs into vectors\n",
        "Y = ys.reshape(total) # Turns pairs into vectors\n",
        "x_hat = pd.DataFrame({'baseline price':X,'footprint':Y}) # Create a dataframe of points to plot\n",
        "y_hat = fitted_model.predict(x_hat) # Fit the model to the points\n",
        "x_hat['Predicted Sales'] = y_hat # Add new variable to the dataframe\n",
        "# Create seaborn plot:\n",
        "this_plot = sns.scatterplot(data=x_hat,x='baseline price',y='footprint',\n",
        "                            hue='Predicted Sales',palette='crest',linewidth=0)\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be5708b-076d-4ba8-92bf-d41a6c43a73e",
      "metadata": {
        "id": "9be5708b-076d-4ba8-92bf-d41a6c43a73e"
      },
      "outputs": [],
      "source": [
        "this_plot = sns.scatterplot(data=df,x='baseline price',y='footprint',hue='baseline sales',\n",
        "                           palette = 'crest')\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d136437d-5575-43f1-834f-756a0a43060b",
      "metadata": {
        "id": "d136437d-5575-43f1-834f-756a0a43060b"
      },
      "source": [
        "## What is a good fit?\n",
        "- In machine learning, one of the main concerns is **external validity** or **generalizability**: Will the model perform well in practice, on data that it wasn't trained to fit?\n",
        "- A **heuristic/rule-of-thumb** is $k^* = \\lfloor \\sqrt{N} \\rfloor$. For the car purchase data, $k^*=21$. But this isn't really data-driven at all.\n",
        "- There are roughly two concerns: Models will overutilize features of the data that might be specific to this data and not be present in the population overall, and model **hyper parameters** like $k$ will tend to **overfit** and focus on feature of the data that are unlikely to appear in a new sample\n",
        "- In general, models that fit well on training data are not guaranteed in any way to fit well on new data (when predictive performance matters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f8969a",
      "metadata": {
        "id": "25f8969a"
      },
      "source": [
        "## Core Ideas of Model Selection\n",
        "- In regression, we start with the **errors** or **residuals** for each observation:\n",
        "\n",
        "    \\begin{alignat*}{1}\n",
        "    e_i = y_i - \\hat{y}_i(x_i,k)\n",
        "    \\end{alignat*}\n",
        "\n",
        "    This quantifies how far off the $i$-th prediction is from the actual value\n",
        "- In classification, we start with the confusion matrix that cross tabulates how close our predictions were to the true values. In cases where $0$ is negative and $1$ is positive, we have:\n",
        "\n",
        "|           |          |                | Actual         |\n",
        "|-----------|----------|----------------|----------------|\n",
        "|           |          | Negative       | Positive       |\n",
        "| Predicted | Negative | True Negative  | False Negative |\n",
        "|           | Positive | False Positive | True Positive  |\n",
        "\n",
        "More generally, we can cross tabulate the predicted with actual values for any number of categories (like vehicle class)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d44de1-ed2a-4586-b48c-bf1acf5058ff",
      "metadata": {
        "id": "e0d44de1-ed2a-4586-b48c-bf1acf5058ff"
      },
      "source": [
        "## Fit and Sum of Squared Error\n",
        "- What is a good fit?\n",
        "- We usually have a distance metric in mind: \"How far are the predictions from the true outcomes, on the validation/test set?\"\n",
        "- So for each observation $i$ in the validation set, we subtract the predicted value from the true one and square it, then sum over all the observations:\n",
        "$$\n",
        "SSE(k) = \\sum_{i=1}^L (y_i - \\hat{y}(x_i,k))^2 = \\sum_{i=1}^L e_i^2\n",
        "$$\n",
        "- The $SSE(k)$ is a really common measure of how far the predicted values are from the true values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64dfd88b-b015-4bff-bd9b-bbad00e0ace3",
      "metadata": {
        "id": "64dfd88b-b015-4bff-bd9b-bbad00e0ace3"
      },
      "source": [
        "## Fitting the Model: Selecting $k$\n",
        "\n",
        "- A simple and effective data-driven strategy is to randomly **split the data** into two sets, a **training set** and a **test/validation set**:\n",
        "  - For a reasonable range of values $k = 1, 2, ..., \\overline{K}$, we make a prediction $\\hat{y}_i$ using the training set for each observation in the test set, $x_i$\n",
        "  - Compute the squared difference from the prediction $\\hat{y}_i$ to the true outcome $y_i$ for each point $i$ in the test set, then sum to get the *sum of squared error (SSE)*\n",
        "  - Pick the $k$ that achieves the lowest SSE\n",
        "- The variable $k$ is our first example of a *hyperparameter*: The data themselves do not prescribe a value of $k$, and naive ways of picking it will lead to bad predictive models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708e8de4-63dd-4d95-9a71-592594d60faa",
      "metadata": {
        "id": "708e8de4-63dd-4d95-9a71-592594d60faa"
      },
      "source": [
        "## Splitting the Sample\n",
        "- `sk` has a `.model_selection` module that contains a `train_test_split` function that makes it easy to split your data\n",
        "- Once the data are split, we use a `for` loop to compute the SSE for the training and test data for each reasonable value of $k$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71098ff0-37c4-4276-8935-b8b65bd8d1af",
      "metadata": {
        "id": "71098ff0-37c4-4276-8935-b8b65bd8d1af"
      },
      "outputs": [],
      "source": [
        "y = df['baseline sales'] # Create target variable vector\n",
        "X = df.loc[:,['baseline price','footprint']] # Create feature matrix\n",
        "X = X.apply(maxmin) # Normalize X\n",
        "\n",
        "# Split the sample:\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables\n",
        "                                                    test_size=.2, # Split the sample 80 train/ 20 test\n",
        "                                                    random_state=100) # For replication purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194e4528-08be-4ef2-887f-f6cbff57a063",
      "metadata": {
        "id": "194e4528-08be-4ef2-887f-f6cbff57a063"
      },
      "outputs": [],
      "source": [
        "k_bar = 200 # Number of k's to try\n",
        "SSE = np.zeros(k_bar) # We'll store the SSE here\n",
        "SSE_on_train = np.zeros(k_bar) # For pedogogical purposes, we'll save the training error\n",
        "\n",
        "for k in range(k_bar):\n",
        "    model = KNeighborsRegressor(n_neighbors=k+1) # Create a sk model for k\n",
        "    fitted_model = model.fit(X_train,y_train) # Train the model on our data\n",
        "    y_hat = fitted_model.predict(X_test) # Predict values for test set\n",
        "    SSE[k] = np.sum( (y_test-y_hat)**2 ) # Save the computed SSE for test set\n",
        "    y_hat = fitted_model.predict(X_train) # Predict values for training set\n",
        "    SSE_on_train[k] = np.sum( (y_train-y_hat)**2 ) # Save the computed SSE\n",
        "\n",
        "SSE_min = np.min(SSE) # Find lowest recorded SSE\n",
        "min_index = np.where(SSE==SSE_min) # Find the indices of SSE that equal the minimum\n",
        "k_star = min_index[0]+1 # Find the optimal value of k; why index+1?\n",
        "print(k_star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db7588b-355d-40a8-92e0-010d1d5fb02a",
      "metadata": {
        "id": "2db7588b-355d-40a8-92e0-010d1d5fb02a"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(0,k_bar),SSE,label='Test SSE') # Plot SSE by k\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('SSE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6192277-e711-4aca-ad5f-f0cb6ab02259",
      "metadata": {
        "id": "a6192277-e711-4aca-ad5f-f0cb6ab02259"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(0,k_bar),SSE_on_train,label='Train') # Plot SSE by k\n",
        "plt.plot(np.arange(0,k_bar),SSE,label='Test') # Plot SSE by k\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('SSE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7df28b56-6e39-4eb2-b151-07639f0d5bce",
      "metadata": {
        "id": "7df28b56-6e39-4eb2-b151-07639f0d5bce"
      },
      "source": [
        "## Training Error vs. Test Error\n",
        "- Why does the training error increase in $k$?\n",
        "- Or, what is the training SSE for $k=1$?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f8975ba-901c-4161-ab55-b59f57152536",
      "metadata": {
        "id": "0f8975ba-901c-4161-ab55-b59f57152536"
      },
      "source": [
        "## Additional Variables\n",
        "- The only reason we only used two variables is so we could visualize how the algorithm was working\n",
        "- You can use as many variables as you want, but in high dimensional spaces, everything is far apart and model performance breaks down"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b14fdd6-44cb-4319-b92b-d2dfdcddc74d",
      "metadata": {
        "id": "0b14fdd6-44cb-4319-b92b-d2dfdcddc74d"
      },
      "source": [
        "## Regression versus Classification\n",
        "- Our first application was a *regression* problem: Predicting a numeric or continuous value, which was sales\n",
        "- Our next application is *classification*: Predicting which values are most likely when the outcome is categorical, which is vehicle category\n",
        "- For example, instead of predicting sales, the goal might be to predict the class or make/model based on characteristics like price, engine size, fuel economy, number of doors, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0baf14a1-7a15-4b3f-9ae5-1e36be5fe3a1",
      "metadata": {
        "id": "0baf14a1-7a15-4b3f-9ae5-1e36be5fe3a1"
      },
      "source": [
        "## Classification Example\n",
        "- To illustrate how classification works, we are going to predict class from baseline price and footprint\n",
        "- With classification, the goal is to either the probability that the new case will belong to each category or predict the most likely category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d703983-b3b7-498d-a0be-01724523730f",
      "metadata": {
        "id": "6d703983-b3b7-498d-a0be-01724523730f"
      },
      "outputs": [],
      "source": [
        "this_plot = sns.scatterplot(data=df,x='baseline price',y='footprint',hue='class')\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b5847a-698f-42b6-bd20-892d1e56a327",
      "metadata": {
        "id": "03b5847a-698f-42b6-bd20-892d1e56a327"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Select features/target variable for analysis:\n",
        "y = df['class']\n",
        "X = df.loc[:,['baseline price','footprint']] # Create feature matrix\n",
        "\n",
        "# Max-min normalize the features:\n",
        "X = X.apply(maxmin)\n",
        "\n",
        "# Fit the model:\n",
        "model = KNeighborsClassifier(n_neighbors=3) # Create a sk model for k=3\n",
        "classifier_model = model.fit(X,y) # Train the model on our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee91d04f-cd03-40ee-a579-facfd61f6f90",
      "metadata": {
        "id": "ee91d04f-cd03-40ee-a579-facfd61f6f90"
      },
      "outputs": [],
      "source": [
        "# Graph of predictor:\n",
        "N_x = 100 # Coarseness of x variable\n",
        "N_y = 100 # Coarseness of y variable\n",
        "total = N_x*N_y # Total number of points to plot\n",
        "grid_x = np.linspace(0,1,N_x) # Create a grid of x values\n",
        "grid_y = np.linspace(0,1,N_y) # Create a grid of y values\n",
        "xs, ys = np.meshgrid(grid_x,grid_y) # Explode grids to all possible pairs\n",
        "X = xs.reshape(total) # Turns pairs into vectors\n",
        "Y = ys.reshape(total) # Turns pairs into vectors\n",
        "x_hat = pd.DataFrame({'baseline price':X,'footprint':Y}) # Create a dataframe of points to plot\n",
        "y_hat = classifier_model.predict(x_hat) # Fit the model to the points\n",
        "x_hat['Predicted Class'] = y_hat # Add new variable to the dataframe\n",
        "# Create seaborn plot:\n",
        "this_plot = sns.scatterplot(data=x_hat,x='baseline price',y='footprint',hue='Predicted Class', linewidth=0)\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32797fbf-42a4-4557-801a-bb5660ca26cb",
      "metadata": {
        "id": "32797fbf-42a4-4557-801a-bb5660ca26cb"
      },
      "source": [
        "## Confusion Matrices\n",
        "- The simplest criteria of success with a classifier is the proportion of cases correctly predicted: The **Accuracy**\n",
        "- To more broadly measure the performance of a classifier, we can cross-tabulate the predicted and true values in a **Confusion Matrix**, and look for patterns in the successes and failures\n",
        "- We'll revisit these ideas later in more depth: Accuracy is not a popular criterion for model success\n",
        "- Like test error, we typically need to splt the sample and compute this on hold-out data, not the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c1ad16c-4b22-4e33-82f9-964f49415640",
      "metadata": {
        "id": "9c1ad16c-4b22-4e33-82f9-964f49415640"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Select data:\n",
        "y = df['class'] # Target variable\n",
        "X = df.loc[:,['baseline price','footprint']] # Create feature matrix\n",
        "X = X.apply(maxmin) # Normalize X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd4d371-6ac0-4e2b-90ed-a32902f5c9c5",
      "metadata": {
        "id": "2dd4d371-6ac0-4e2b-90ed-a32902f5c9c5"
      },
      "outputs": [],
      "source": [
        "## Split the sample:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables\n",
        "                                                    test_size=.5, # Split the sample 80 train/ 20 test\n",
        "                                                    random_state=200) # For replication purposes\n",
        "N_train = len(y_train)\n",
        "N_test = len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a842d4-0b34-4f74-ab1f-272206cee976",
      "metadata": {
        "id": "48a842d4-0b34-4f74-ab1f-272206cee976"
      },
      "outputs": [],
      "source": [
        "## Solve for k that maximizes accuracy:\n",
        "k_bar = 100 # Number of k's to try\n",
        "Acc = np.zeros(k_bar) # We'll store the accuracy here\n",
        "\n",
        "for k in range(k_bar):\n",
        "    model = KNeighborsClassifier(n_neighbors=k+1) # Create a sk model for k\n",
        "    fitted_model = model.fit(X_train.values,y_train) # Train the model on our data\n",
        "    y_hat = fitted_model.predict(X_test.values) # Predict values for test set\n",
        "    Acc[k] = np.sum( y_hat == y_test )/N_test # Accuracy on testing data\n",
        "\n",
        "Acc_max = np.max(Acc) # Find highest recorded Accuracy\n",
        "max_index = np.where(Acc==Acc_max) # Find the indices that equal the maximum\n",
        "k_star = max_index[0]+1 # Find the optimal value of k; why index+1?\n",
        "print(k_star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "372fe07d-c55c-4623-91bb-2d72177f1bfa",
      "metadata": {
        "id": "372fe07d-c55c-4623-91bb-2d72177f1bfa"
      },
      "outputs": [],
      "source": [
        "## Fit optimal model:\n",
        "model = KNeighborsClassifier(n_neighbors=k_star[0]) # Create a sk model for k\n",
        "fitted_model = model.fit(X_train.values,y_train) # Train the model on our data\n",
        "y_hat = fitted_model.predict(X_test.values) # Predict values for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f40040-4176-4c48-a92a-7dc96cccd31e",
      "metadata": {
        "id": "b9f40040-4176-4c48-a92a-7dc96cccd31e"
      },
      "outputs": [],
      "source": [
        "## Accuracy plot:\n",
        "plt.plot(np.arange(0,k_bar),Acc,label='Accuracy')\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Test Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75b0ce7-b3ff-478f-8cb3-241a76e4ba58",
      "metadata": {
        "id": "b75b0ce7-b3ff-478f-8cb3-241a76e4ba58"
      },
      "outputs": [],
      "source": [
        "## Confusion matrix:\n",
        "pd.crosstab(y_test,y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ffd9897-8a3d-47ce-bfd5-4d1290e90d71",
      "metadata": {
        "id": "9ffd9897-8a3d-47ce-bfd5-4d1290e90d71"
      },
      "source": [
        "- Notice how, because of the random train/test split, no station wagons or vans made it into the training data, so the algorithm only predicts SUV, car, and truck\n",
        "- Despite that, it predicts that station wagons are most likely to be cars and vans are most likely to be trucks: It still does an OK job of guessing what the vehicle is closest to"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "860065a3-6ea7-4a15-a7c9-8c58555c72cd",
      "metadata": {
        "id": "860065a3-6ea7-4a15-a7c9-8c58555c72cd"
      },
      "source": [
        "## Other Examples\n",
        "- heart_failure.csv: `DEATH_EVENT` is that the patient died, and all of the other varibles are potential predictors\n",
        "- airbnb_hw.csv: Price as a function of characteristics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50555a0d-4da6-4bee-94a7-d65234241dba",
      "metadata": {
        "id": "50555a0d-4da6-4bee-94a7-d65234241dba"
      },
      "source": [
        "## Issues with $k$-NN\n",
        "- This is a purely inductive/blackbox prediction algorithm: We find the $k$ cases that are \"most similar\" to $\\hat{x}$ in terms of distance, and guess the average of the outcomes for those points\n",
        "- The choice of $d$ is crucial but apparently arbitrary\n",
        "- For regression, it returns a *point prediction*, $\\hat{y}$, and we might want to allow for more uncertainty in our predictions\n",
        "- It is unclear how to interpret the impact of individual variables on predictions because of the local smoothing\n",
        "- The local smoothing is \"hard\" like a histogram and not \"smooth\" like a kernel density; this leads to instability in predictions\n",
        "- $k$NN is our \"chunkiest\", \"most local\" predictive algorithm, so it is often good for imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd849970-ec92-4d59-85d9-749ad366f813",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "dd849970-ec92-4d59-85d9-749ad366f813"
      },
      "source": [
        "## Main Take-aways\n",
        "There is a lot in this lecture, if you are new to predictive modeling. We will revist most of it in future classes, but here are some key ideas:\n",
        "- Euclidean Distance\n",
        "- Regression versus Classification\n",
        "- Feature Scaling/Normalization\n",
        "- Splitting the Sample and Hyperparameter Fitting\n",
        "- Sum of Squared Error\n",
        "- kNN regression and classification\n",
        "- The `.apply(fcn)` method"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}